{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation:\n",
        "It installs the necessary libraries transformers and torch using pip."
      ],
      "metadata": {
        "id": "yinksl09YJKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "VD5ZPrhSYBe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports:\n",
        "It imports required modules including DistilBertTokenizer and DistilBertModel from the transformers library, torch, Counter from the collections module, and re for regular expressions."
      ],
      "metadata": {
        "id": "lxXA7D-OYqHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer\n",
        "from collections import Counter\n",
        "import re"
      ],
      "metadata": {
        "id": "1mEfe2PjYx0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Sample Abstracts:\n",
        "Two sample abstracts are provided to demonstrate keyword extraction."
      ],
      "metadata": {
        "id": "9vuNtwW1Y0K3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample abstracts\n",
        "abstract1 = \"This paper presents a novel approach to keyword extraction using transformer-based models. We demonstrate the effectiveness of our method on a diverse set of research papers.\"\n",
        "abstract2 = \"In this study, we explore the application of deep learning techniques for keyword extraction from text. Our experiments show promising results on various datasets.\""
      ],
      "metadata": {
        "id": "n1twyL46ZN6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Relevant Keywords:\n",
        " A list named relevant_keywords is defined, which contains keywords relevant to data science and machine learning."
      ],
      "metadata": {
        "id": "eKqKuaNIZRHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Relevant keywords related to data science and machine learning\n",
        "relevant_keywords = [\"data\", \"science\", \"machine\", \"learning\", \"deep\", \"techniques\", \"research\", \"papers\", \"datasets\"]"
      ],
      "metadata": {
        "id": "bLJ93wMQaExX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtering and Sorting Relevant Tokens:\n",
        "After counting the occurrences of each token, the code filters tokens based on their relevance to data science and machine learning. Tokens that match the keywords in relevant_keywords are retained. Then, these relevant tokens are sorted by their frequency in descending order."
      ],
      "metadata": {
        "id": "ENtpiPEsaGSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the abstracts\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "tokenized_abs1 = tokenizer.tokenize(abstract1)\n",
        "tokenized_abs2 = tokenizer.tokenize(abstract2)\n",
        "\n",
        "# Concatenate the tokenized abstracts\n",
        "all_tokens = tokenized_abs1 + tokenized_abs2\n",
        "\n",
        "# Remove punctuation tokens\n",
        "all_tokens = [token for token in all_tokens if re.match(r'^\\w', token)]\n",
        "\n",
        "# Convert the tokens to lowercase\n",
        "all_tokens = [token.lower() for token in all_tokens]\n",
        "\n",
        "# Count token occurrences\n",
        "token_counts = Counter(all_tokens)\n",
        "\n",
        "# Filter tokens based on relevance to data science and machine learning\n",
        "relevant_tokens = [(token, count) for token, count in token_counts.items() if token in relevant_keywords]\n",
        "\n",
        "# Sort relevant tokens by frequency\n",
        "sorted_tokens = sorted(relevant_tokens, key=lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "id": "i-pYrsmoabNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keyword Extraction and Printing:\n",
        "The top 5 most frequent relevant tokens are extracted as keywords. Finally, it prints the top keywords related to data science and machine learning extracted from the provided abstracts."
      ],
      "metadata": {
        "id": "qmK_u7fXacES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Extract top 5 keywords\n",
        "top_keywords = [token[0] for token in sorted_tokens[:5]]\n",
        "print(\"Top keywords related to data science and machine learning:\", top_keywords)\n"
      ],
      "metadata": {
        "id": "GoavPcOSOyAT",
        "outputId": "636efef1-7688-46d3-8d58-6bb80d3bb11d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top keywords related to data science and machine learning: ['research', 'papers', 'deep', 'learning', 'techniques']\n"
          ]
        }
      ]
    }
  ]
}